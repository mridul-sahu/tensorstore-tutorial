{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzZatgoEUsBnuSnLRuP8Gi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/tensorstore-tutorial/blob/main/TensorStore_Tutorial_From_Zero_to_Hero_%F0%9F%9A%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorStore Tutorial: From Zero to Hero** üöÄ\n",
        "\n",
        "Welcome! This notebook is your complete guide to TensorStore. We'll start from the absolute basics and build up to advanced concepts, explaining *why* we need each feature along the way.\n",
        "\n",
        "---\n",
        "\n",
        "## **Module 1: The Problem with Big Data & Your First TensorStore**\n",
        "\n",
        "### **The \"Why\": The Limits of In-Memory Computing** ü§î\n",
        "\n",
        "Imagine you're a neuroscientist with a 5-terabyte dataset of high-resolution brain scans. Your laptop probably has 16 or 32 GB of RAM. What happens when you try to load your data?\n",
        "\n",
        "```python\n",
        "# import numpy as np\n",
        "# huge_array = np.load('my_5_terabyte_dataset.npy') # <-- This will crash!\n",
        "```\n",
        "\n",
        "The program will immediately crash with an OutOfMemoryError. This is the fundamental problem that tools like TensorStore are designed to solve.\n",
        "\n",
        "### **Key Pain Points of \"Big Data\":**\n",
        "\n",
        "1. **Data Exceeds RAM**: You cannot load the entire dataset into memory at once.\n",
        "\n",
        "2. **You Only Need a Piece**: You don't want to analyze all 5TB at once. You might only need to inspect a single patient's scan, or even just a small region of that scan.\n",
        "\n",
        "3. **Data Lives Elsewhere**: The data is almost never on your local machine. It's usually stored in the cloud (like Google Cloud Storage or Amazon S3).\n",
        "\n",
        "**TensorStore's Solution**: TensorStore acts as a \"universal remote\" for your array data. It provides a familiar NumPy-like interface (array[x, y, z]) but intelligently reads only the specific bytes you request from the underlying storage, whether it's a local file or a cloud object."
      ],
      "metadata": {
        "id": "0-fLt080pgCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtE8PqxdqF0_",
        "outputId": "f78b2f24-887a-40a5-d75e-b4f997f3e77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (0.1.74)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from tensorstore) (2.0.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorstore) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The \"Spec\": Your Data's Recipe Card** üìù\n",
        "\n",
        "Every TensorStore begins with a **`spec`** (specification). This is a simple Python dictionary that acts as a recipe card, telling TensorStore everything it needs to know about your data's structure and location.\n",
        "\n",
        "* `driver`: The storage **format**, like `zarr` (a popular, modern format for chunked arrays).\n",
        "* `kvstore`: Short for \"key-value store\", this dictionary specifies the **location** where the `zarr` chunks will be saved. We'll start with a `file` driver to save to our local disk.\n",
        "* `metadata`: A dictionary containing the crucial properties of the array:\n",
        "    * `dtype`: The data type, specified in Zarr's precise format (e.g., `'|u1'` for `uint8`).\n",
        "    * `shape`: The dimensions of the full array.\n",
        "    * `chunks`: The dimensions of the small blocks the array is broken into."
      ],
      "metadata": {
        "id": "glAWBbSmqI7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorstore as ts\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the recipe for our local data store.\n",
        "# This doesn't create any files yet, it's just a plan.\n",
        "spec = {\n",
        "    'driver': 'zarr', # The format is Zarr\n",
        "    'kvstore': {\n",
        "        'driver': 'file', # The location is the local filesystem\n",
        "        'path': '/tmp/my_zarr_dataset'\n",
        "    },\n",
        "    'metadata': {\n",
        "        'dtype': '|u1',\n",
        "        'shape': [100, 256, 256],  # 100 \"images\", each 256x256 pixels\n",
        "        'chunks': [16, 64, 64] # For Zarr, the key is 'chunks'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Our TensorStore Spec:\")\n",
        "print(spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MrKWT_fqV6e",
        "outputId": "e60acfd6-9f35-44b4-ab7c-bb70bd2306f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our TensorStore Spec:\n",
            "{'driver': 'zarr', 'kvstore': {'driver': 'file', 'path': '/tmp/my_zarr_dataset'}, 'metadata': {'dtype': '|u1', 'shape': [100, 256, 256], 'chunks': [16, 64, 64]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **\"Hello, World!\": Creating Your First Store**\n",
        "\n",
        "Now that we have the recipe, let's bring our data store to life. We use `ts.open()` and pass it our `spec`. We also include two important flags:\n",
        "\n",
        "* `create=True`: This tells TensorStore to create the dataset if it doesn't already exist.\n",
        "* `delete_existing=True`: This will clear any old data at that path, ensuring we start fresh."
      ],
      "metadata": {
        "id": "rIZEQIL5qaEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Opening the Store\n",
        "\n",
        "# This operation will create the directory /tmp/my_zarr_dataset\n",
        "# and write some metadata files inside it.\n",
        "store = ts.open(spec, create=True, delete_existing=True).result()\n",
        "\n",
        "print(\"Successfully created a TensorStore object!\")\n",
        "print(\"\\n--- Store Details ---\")\n",
        "print(\"Data Type (dtype):\", store.dtype)\n",
        "print(\"Dimensions (ndim):\", store.ndim)\n",
        "print(\"Shape:\", store.shape)\n",
        "# The 'domain' includes the shape and coordinate system information\n",
        "print(\"Domain:\", store.domain)\n",
        "print(\"Schema\", store.schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dPmuo_Sqaxx",
        "outputId": "235fa0c6-d685-4dd0-bd86-fc5fa0621019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created a TensorStore object!\n",
            "\n",
            "--- Store Details ---\n",
            "Data Type (dtype): dtype(\"uint8\")\n",
            "Dimensions (ndim): 3\n",
            "Shape: (100, 256, 256)\n",
            "Domain: { [0, 100*), [0, 256*), [0, 256*) }\n",
            "Schema Schema({\n",
            "  'chunk_layout': {\n",
            "    'grid_origin': [0, 0, 0],\n",
            "    'inner_order': [0, 1, 2],\n",
            "    'read_chunk': {'shape': [16, 64, 64]},\n",
            "    'write_chunk': {'shape': [16, 64, 64]},\n",
            "  },\n",
            "  'codec': {\n",
            "    'compressor': {\n",
            "      'blocksize': 0,\n",
            "      'clevel': 5,\n",
            "      'cname': 'lz4',\n",
            "      'id': 'blosc',\n",
            "      'shuffle': -1,\n",
            "    },\n",
            "    'driver': 'zarr',\n",
            "    'filters': None,\n",
            "  },\n",
            "  'domain': {\n",
            "    'exclusive_max': [[100], [256], [256]],\n",
            "    'inclusive_min': [0, 0, 0],\n",
            "  },\n",
            "  'dtype': 'uint8',\n",
            "  'rank': 3,\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 2: Core Mechanics - Reading, Writing, and Asynchronous Magic**\n",
        "\n",
        "### **The \"Why\": The I/O Bottleneck & The Power of Chunking** üê¢üí®\n",
        "\n",
        "Think of your 5TB dataset on disk as a giant library. If you wanted one sentence from one book, you wouldn't copy the entire library to your desk first. You'd go to the right shelf, pull the right book, and read just that one sentence.\n",
        "\n",
        "This is the core idea behind **chunking**. TensorStore breaks your giant array into small, independent blocks on disk, as defined by the `chunk_layout` in our spec. When you ask for a small piece of the array, TensorStore calculates exactly which chunks it needs and reads only them. This is why it's so efficient.\n",
        "\n",
        "Furthermore, reading from a disk or a network is thousands of times slower than reading from RAM. This delay is the **I/O bottleneck**. To prevent your entire program from freezing while waiting, TensorStore performs all I/O operations **asynchronously**."
      ],
      "metadata": {
        "id": "KWj_X8X1opjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Writing Data to the Store** ‚úçÔ∏è\n",
        "\n",
        "Writing data is as intuitive as assigning values to a NumPy array. You use standard slicing to specify the region you want to write to and then call the `.write()` method. Because of chunking, you don't have to worry about the rest of the array; TensorStore will locate and modify only the affected chunks."
      ],
      "metadata": {
        "id": "LXnecx-0pNFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Writing a Data Patch\n",
        "\n",
        "# First, let's reopen our existing store. We don't need 'create=True' anymore.\n",
        "# We can just pass the spec directly.\n",
        "spec['create'] = False\n",
        "spec['open'] = True\n",
        "store = ts.open(spec).result()\n",
        "\n",
        "# Create a small 64x64 numpy array (a \"patch\") filled with the value 255 (white).\n",
        "patch_data = np.full(shape=[64, 64], fill_value=255, dtype='uint8')\n",
        "\n",
        "# Write this patch into the 50th \"image\" (index 49) at a specific location.\n",
        "# Note: The write operation is queued in the background.\n",
        "write_future = store[49, 100:164, 100:164].write(patch_data)\n",
        "\n",
        "# To ensure the write is complete before we move on, we call .result().\n",
        "# This \"waits\" for the future to resolve.\n",
        "write_future.result()\n",
        "\n",
        "print(\"‚úÖ Patch written successfully!\")\n",
        "\n",
        "# Let's see what was created on disk. Zarr creates metadata files like '.zarray'.\n",
        "print(\"\\n--- Files on Disk ---\")\n",
        "!ls /tmp/my_zarr_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ8-DxW7pOtD",
        "outputId": "296c23de-95d0-4fdc-dfb5-a70026e24735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Patch written successfully!\n",
            "\n",
            "--- Files on Disk ---\n",
            "3.1.1  3.1.2  3.2.1  3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reading Data from the Store** üßê\n",
        "\n",
        "Reading works the same way. You specify the region you want with slicing and call the `.read()` method. This will return a `Future` object that, once its result is ready, will contain a standard NumPy array with your requested data."
      ],
      "metadata": {
        "id": "DU02fO62pRwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Reading and Verifying\n",
        "\n",
        "# Read the same 64x64 region back from the store.\n",
        "read_future = store[49, 100:164, 100:164].read()\n",
        "\n",
        "# Get the result, which will be a NumPy array.\n",
        "read_data = read_future.result()\n",
        "\n",
        "print(\"Shape of read data:\", read_data.shape)\n",
        "print(\"Is the data we read back the same as our original patch?\", np.array_equal(patch_data, read_data))\n",
        "\n",
        "# What happens if we read from a location we never wrote to?\n",
        "# TensorStore returns a block of zeros (the default fill value).\n",
        "empty_region = store[0, 0:5, 0:5].read().result()\n",
        "\n",
        "print(\"\\nAn unwritten region contains:\")\n",
        "print(empty_region)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5HtLC27pT_D",
        "outputId": "a727f4fe-7a66-4fb8-dc53-9b0cbd8ec013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of read data: (64, 64)\n",
            "Is the data we read back the same as our original patch? True\n",
            "\n",
            "An unwritten region contains:\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Asynchronous Magic (`Future`)** ‚ö°\n",
        "\n",
        "We've been using `.result()` to wait for a single operation. But what if you have hundreds of writes to perform? Waiting for each one sequentially would be slow and defeat the purpose of asynchronous I/O.\n",
        "\n",
        "When you call `.write()` or `.read()`, TensorStore gives you a **`Future`** object *immediately*. This object is a promise that the work will be done. Your Python script can continue running and doing other things. You can gather many of these `Future` objects and then wait for all of them to complete at once. This allows TensorStore to schedule all your I/O operations in the most efficient way possible in the background."
      ],
      "metadata": {
        "id": "ZUZKX4empWAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Working with Futures\n",
        "\n",
        "import time\n",
        "\n",
        "# Let's write three different patches to three different locations.\n",
        "patch_1 = np.full([10, 10], 1, dtype='uint8')\n",
        "patch_2 = np.full([10, 10], 2, dtype='uint8')\n",
        "patch_3 = np.full([10, 10], 3, dtype='uint8')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# We start the write operations but DON'T wait for them individually.\n",
        "# This queues them up.\n",
        "future1 = store[0, 0:10, 0:10].write(patch_1)\n",
        "future2 = store[1, 0:10, 0:10].write(patch_2)\n",
        "future3 = store[2, 0:10, 0:10].write(patch_3)\n",
        "\n",
        "print(\"All three write operations have been queued.\")\n",
        "\n",
        "# Now, we wait for each one to finish by calling .result() on each.\n",
        "# This ensures all are complete before we proceed.\n",
        "future1.result()\n",
        "future2.result()\n",
        "future3.result()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nAll three writes completed in {end_time - start_time:.4f} seconds.\")\n",
        "\n",
        "# Let's verify one of them.\n",
        "print(\"Data from the second write:\", store[1, 5, 5].read().result())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEKwI8IypW98",
        "outputId": "c61c17af-c4c4-4b65-f00a-a9aa84685f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All three write operations have been queued.\n",
            "\n",
            "All three writes completed in 0.0293 seconds.\n",
            "Data from the second write: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 3: Moving to the Cloud** ‚òÅÔ∏è\n",
        "\n",
        "### **The \"Why\": Science and ML Live in the Cloud**\n",
        "\n",
        "So far, we've only worked with data on our local disk. But modern large-scale datasets for science and machine learning almost always live in cloud storage (like Google Cloud Storage, Amazon S3, or Azure Blob Storage).\n",
        "\n",
        "A key challenge is accessing this data efficiently without rewriting your analysis code every time you switch storage providers. You need a tool that can speak the language of cloud storage natively. This is where TensorStore truly shines."
      ],
      "metadata": {
        "id": "H8qvxmsYqrSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The `kvstore` Abstraction: Separating \"What\" from \"Where\"**\n",
        "\n",
        "TensorStore uses a brilliant abstraction to achieve this. It separates the **data format** from the **storage location**.\n",
        "\n",
        "1.  **The `driver`:** This defines the *format* of your array on disk. Popular formats for chunked arrays include `zarr` and `n5`. This is the \"what\".\n",
        "2.  **The `kvstore`:** This stands for \"key-value store\". It's a field inside the `spec` that tells the `driver` *where* to store its chunks. This `kvstore` has its own driver, like `file` for local disk or `gcs` for Google Cloud Storage. This is the \"where\".\n",
        "\n",
        "By nesting the `kvstore` spec inside the main spec, you can mix and match formats and storage locations with incredible flexibility."
      ],
      "metadata": {
        "id": "n2tg9L_Pqy7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Revisiting the `spec` for Cloud Storage\n",
        "\n",
        "# Let's see a spec for a Zarr-formatted array on the local filesystem.\n",
        "# The 'zarr' driver uses a 'kvstore' to know where to save its files.\n",
        "local_zarr_spec = {\n",
        "    'driver': 'zarr',\n",
        "    'kvstore': {\n",
        "        'driver': 'file',\n",
        "        'path': '/tmp/my_local_zarr_dataset'\n",
        "    },\n",
        "    'metadata': {\n",
        "        'shape': [1000, 1000],\n",
        "        'dtype': 'float32',\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"--- Spec for a Local Zarr array ---\")\n",
        "print(local_zarr_spec)\n",
        "\n",
        "\n",
        "# Now, to move this to Google Cloud Storage, we only change the 'kvstore'.\n",
        "# The top-level 'driver' remains 'zarr'.\n",
        "#\n",
        "# THIS CELL IS AN EXAMPLE AND WILL NOT RUN unless you have a GCS bucket\n",
        "# and have authenticated your environment.\n",
        "gcs_zarr_spec = {\n",
        "    'driver': 'zarr',\n",
        "    'kvstore': {\n",
        "        'driver': 'gcs',\n",
        "        'bucket': 'your-gcs-bucket-name' # <-- You would change this\n",
        "    },\n",
        "    'path': 'path/to/your/data', # <-- And this path within the bucket\n",
        "    'metadata': {\n",
        "        'shape': [1000, 1000],\n",
        "        'dtype': 'float32',\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\\n--- Spec for a Zarr array on Google Cloud Storage ---\")\n",
        "print(gcs_zarr_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7pU04qhrHU0",
        "outputId": "52cfebe1-a0a7-4389-fd0e-36accf835504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spec for a Local Zarr array ---\n",
            "{'driver': 'zarr', 'kvstore': {'driver': 'file', 'path': '/tmp/my_local_zarr_dataset'}, 'metadata': {'shape': [1000, 1000], 'dtype': 'float32'}}\n",
            "\n",
            "\n",
            "--- Spec for a Zarr array on Google Cloud Storage ---\n",
            "{'driver': 'zarr', 'kvstore': {'driver': 'gcs', 'bucket': 'your-gcs-bucket-name'}, 'path': 'path/to/your/data', 'metadata': {'shape': [1000, 1000], 'dtype': 'float32'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Power of Portability**\n",
        "\n",
        "Notice what happened in the cell above. The only part that changed was the `kvstore` block.\n",
        "\n",
        "This is the most powerful takeaway: **your Python code for reading and writing data does not change at all.**\n",
        "\n",
        "```python\n",
        "# This code works IDENTICALLY for both local and cloud stores:\n",
        "#\n",
        "# store = ts.open(local_zarr_spec, create=True).result()\n",
        "# store[0:100, 0:100].write(some_numpy_array).result()\n",
        "#\n",
        "# store = ts.open(gcs_zarr_spec, create=True).result()\n",
        "# store[0:100, 0:100].write(some_numpy_array).result()\n",
        "```"
      ],
      "metadata": {
        "id": "aE-MUMFxrJ11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 4: Advanced Superpowers - Views and Transactions**\n",
        "\n",
        "### **The \"Why\": The Cost of Duplication and The Danger of Concurrency**\n",
        "\n",
        "As your data workflows become more complex, two new challenges emerge:\n",
        "\n",
        "1.  **Transformations:** What if you need to work with a transformed version of your data? For example, a down-sampled version for visualization, or a transposed view for a specific algorithm. Creating a full copy for each transformation is incredibly expensive in terms of storage and computation.\n",
        "2.  **Concurrency:** What if multiple programs‚Äîor even multiple threads in the same program‚Äîtry to write to the same dataset at the same time? This can lead to a \"race condition,\" where the final state of the data is corrupted and incorrect because the operations interleave in an unpredictable way.\n",
        "\n",
        "TensorStore provides elegant solutions for both of these problems: **Virtual Views** and **Transactions**."
      ],
      "metadata": {
        "id": "KnI9u57mrXLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Virtual Views üëì: Zero-Cost Transformations**\n",
        "\n",
        "A **virtual view** is one of TensorStore's most powerful features. It's like putting a special \"lens\" on your data that transforms it on the fly, without ever creating a second copy in storage. The transformations are applied as you read or write data through the view.\n",
        "\n",
        "This is extremely efficient. You can create views that select, slice, reverse, or reorder dimensions, and the only cost is a tiny amount of memory for the view object itself."
      ],
      "metadata": {
        "id": "9Hd00NTlrbsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reopen the store from the previous modules\n",
        "spec = {\n",
        "    'driver': 'zarr',\n",
        "    'kvstore': {'driver': 'file', 'path': '/tmp/my_zarr_dataset'},\n",
        "    'open': True\n",
        "}\n",
        "store = ts.open(spec).result()\n",
        "\n",
        "# Create our 2x3 pixel letter 'L' patch\n",
        "l_patch = np.array([[255, 0, 0],\n",
        "                    [255, 255, 255]], dtype='uint8')\n",
        "\n",
        "\n",
        "# Apply the full transformation (slice and flip) in a single operation\n",
        "# directly on the base `store` object for both writing and reading.\n",
        "\n",
        "# 1. WRITE to the transformed location.\n",
        "# The slice [255:252:-1] is the flipped version of [0:3].\n",
        "store[0, 0:2, 255:252:-1].write(l_patch).result()\n",
        "print(\"‚úÖ Wrote an 'L' patch directly to the flipped location in the store.\")\n",
        "\n",
        "\n",
        "# 2. READ from the exact same transformed location to verify.\n",
        "read_data = store[0, 0:2, 255:252:-1].read().result()\n",
        "print(\"\\nData read back from the transformed location:\")\n",
        "print(read_data)\n",
        "\n",
        "# 3. VERIFY that what we read is what we wrote.\n",
        "print(\"\\nIs the data we read the same as the patch we wrote?\", np.array_equal(l_patch, read_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kb8vRaoreD0",
        "outputId": "ddabdf3c-17ba-419e-8645-479dd5b1c19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Wrote an 'L' patch directly to the flipped location in the store.\n",
            "\n",
            "Data read back from the transformed location:\n",
            "[[255   0   0]\n",
            " [255 255 255]]\n",
            "\n",
            "Is the data we read the same as the patch we wrote? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transactions ‚öõÔ∏è: Ensuring Data Integrity**\n",
        "\n",
        "A **transaction** is a sequence of operations that is guaranteed to be **atomic**. Atomicity is an all-or-nothing promise: either every single operation within the transaction succeeds, or the entire set of operations is rolled back as if it never happened.\n",
        "\n",
        "This is crucial for preventing data corruption. For example, in a \"read-modify-write\" cycle, a transaction ensures that no other process can modify the data between the time you read it and the time you write your changes back. TensorStore achieves this using an optimistic concurrency model, which is highly efficient for most workloads."
      ],
      "metadata": {
        "id": "pcltOCU7rkIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorstore as ts\n",
        "\n",
        "# Let's reopen the store\n",
        "spec = {\n",
        "    'driver': 'zarr',\n",
        "    'kvstore': {'driver': 'file', 'path': '/tmp/my_zarr_dataset'},\n",
        "    'open': True\n",
        "}\n",
        "store = ts.open(spec).result()\n",
        "\n",
        "# Let's imagine we want to \"increment\" a value in our store.\n",
        "pixel_coord = (10, 20, 30)\n",
        "\n",
        "# 1. Create an instance of the Transaction class.\n",
        "txn = ts.Transaction()\n",
        "\n",
        "# 2. Open the store within the context of the transaction.\n",
        "#    All subsequent operations on `store_in_txn` are part of this transaction.\n",
        "store_in_txn = store.with_transaction(txn)\n",
        "\n",
        "# 3. Perform the read-modify-write sequence.\n",
        "#    These operations are staged within the transaction but not yet committed.\n",
        "current_value_future = store_in_txn[pixel_coord].read()\n",
        "current_value = current_value_future.result()\n",
        "new_value = current_value + 1\n",
        "write_future = store_in_txn[pixel_coord].write(new_value)\n",
        "\n",
        "# Wait for the write to be staged.\n",
        "write_future.result()\n",
        "\n",
        "# 4. Explicitly commit the transaction.\n",
        "#    This makes all the changes (the increment) final and atomic.\n",
        "txn.commit_async().result()\n",
        "\n",
        "print(f\"Transaction committed. Incremented pixel at {pixel_coord}.\")\n",
        "\n",
        "# Let's verify the new value outside the transaction\n",
        "final_value = store[pixel_coord].read().result()\n",
        "print(f\"Verified new value: {final_value}\")\n",
        "\n",
        "# If you run this cell again, the value will correctly increment to 2."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V-4TOwprrYc",
        "outputId": "b05b0f73-fe7c-41b0-f636-5be741f9f16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transaction committed. Incremented pixel at (10, 20, 30).\n",
            "Verified new value: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 5: The Grand Finale - How and Why Orbax Uses TensorStore**\n",
        "\n",
        "### **The \"Why\": The ML Checkpointing Challenge** üíæ\n",
        "\n",
        "In modern machine learning, models can be enormous, with billions of parameters. During training, you need to periodically save the state of your model (the \"weights\" or \"parameters\") to a file. This is called **checkpointing**.\n",
        "\n",
        "Checkpointing is critical for two reasons:\n",
        "1.  **Fault Tolerance:** If your training job crashes (which is common in large-scale environments), you can resume from the last saved checkpoint instead of starting over.\n",
        "2.  **Inference:** Once the model is trained, you use the final checkpoint to run it on new data.\n",
        "\n",
        "The challenge is that a model isn't one big array; it's a complex structure (often called a `PyTree` in JAX) containing thousands of individual arrays. Saving each one as a separate file is incredibly slow and inefficient, especially on cloud file systems. This is the exact problem Orbax is designed to solve, using TensorStore as its engine."
      ],
      "metadata": {
        "id": "qkHU8BpZspdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Orbax: A Checkpointing Library Powered by TensorStore**\n",
        "\n",
        "**Orbax** is a library designed specifically for robust and performant checkpointing in JAX. Instead of reinventing the wheel for writing array data to storage, Orbax delegates this critical task to TensorStore.\n",
        "\n",
        "Think of it this way:\n",
        "* **You** tell Orbax to save your JAX model.\n",
        "* **Orbax** figures out the structure of your model and organizes the checkpointing process.\n",
        "* **TensorStore** does the heavy lifting of efficiently writing each individual array from your model to the physical storage (local disk or cloud)."
      ],
      "metadata": {
        "id": "p8qoVVtnsu_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we need to install Orbax and JAX.\n",
        "!pip install orbax-checkpoint jax jaxlib"
      ],
      "metadata": {
        "id": "TLKD4z4_Aob3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from orbax import checkpoint as ocp\n",
        "import os\n",
        "\n",
        "# 1. Create a sample JAX PyTree (like a simple ML model's parameters).\n",
        "# This is a dictionary of named arrays.\n",
        "my_model_params = {\n",
        "    'layer1': {\n",
        "        'weights': jnp.ones((128, 64)),\n",
        "        'bias': jnp.zeros((64,))\n",
        "    },\n",
        "    'layer2': {\n",
        "        'weights': jnp.ones((64, 32)),\n",
        "        'bias': jnp.zeros((32,))\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Set up an Orbax Checkpointer. This object manages saving and restoring.\n",
        "# We're telling it to save to a directory named '/tmp/my_orbax_checkpoint/'.\n",
        "ckpt_dir = '/tmp/my_orbax_checkpoint/'\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "\n",
        "# 3. Save the PyTree. Orbax uses TensorStore under the hood to write the data.\n",
        "checkpointer.save(os.path.join(ckpt_dir, '1'), my_model_params)\n",
        "\n",
        "checkpointer.wait_until_finished()\n",
        "\n",
        "print(f\"‚úÖ Checkpoint saved to {ckpt_dir}\")\n",
        "!ls -R {ckpt_dir}\n",
        "\n",
        "# 4. Restore the PyTree. Orbax uses TensorStore to read the data back.\n",
        "restored_params = checkpointer.restore(os.path.join(ckpt_dir, '1'))\n",
        "print(\"\\nRestored successfully! Bias of layer 2:\", restored_params['layer2']['bias'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omu7nDVKswoF",
        "outputId": "714e2929-ec45-4631-ab3a-b5accccc4f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Checkpoint saved to /tmp/my_orbax_checkpoint/\n",
            "/tmp/my_orbax_checkpoint/:\n",
            "1\n",
            "\n",
            "/tmp/my_orbax_checkpoint/1:\n",
            "array_metadatas       d\t\t      _METADATA        _sharding\n",
            "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n",
            "\n",
            "/tmp/my_orbax_checkpoint/1/array_metadatas:\n",
            "process_0\n",
            "\n",
            "/tmp/my_orbax_checkpoint/1/d:\n",
            "66f0d80ee063dbc7b43ac58d5c68cfb5\n",
            "\n",
            "/tmp/my_orbax_checkpoint/1/ocdbt.process_0:\n",
            "d  manifest.ocdbt\n",
            "\n",
            "/tmp/my_orbax_checkpoint/1/ocdbt.process_0/d:\n",
            "1a46bfaecca1c3a8982f784f315d0a27  77b9243ba07118f7b62c7b543e74baa3\n",
            "1ba2075dc8422d37c6f1d17b20ff38bc  ab9b36aec28474627571e7cbc4592f06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Restored successfully! Bias of layer 2: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Under the Hood: What Is Orbax Really Doing?**\n",
        "\n",
        "When you call `checkpointer.save(...)`, Orbax doesn't just dump the files. It performs a sophisticated dance with TensorStore:\n",
        "\n",
        "1.  **Iteration:** Orbax traverses your PyTree (`my_model_params`).\n",
        "2.  **Spec Generation:** For each array (like `'weights'` and `'bias'`), Orbax creates a TensorStore `spec`. This spec tells TensorStore where to save the array within the checkpoint directory and what its properties are.\n",
        "3.  **Asynchronous Writes:** Orbax then uses TensorStore's asynchronous API to issue write commands for all arrays, allowing them to be written in parallel and with maximum efficiency.\n",
        "4.  **Specialized Driver:** For cloud storage, Orbax often uses a special TensorStore driver called `ocdbt` (Optimized Concurrent Database Transaction). This driver is designed to aggregate many small array chunks into fewer, larger files, which dramatically improves performance on cloud filesystems that penalize many small I/O operations.\n",
        "\n",
        "Essentially, Orbax acts as the smart manager, while TensorStore is the high-performance worker handling the data."
      ],
      "metadata": {
        "id": "iC21wl0Js5wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Advanced Topic: Reading an Orbax Checkpoint with Pure TensorStore**\n",
        "\n",
        "You've seen how Orbax provides a convenient `restore()` method. But what if you don't have Orbax, or you want to inspect a single array from a checkpoint for debugging?\n",
        "\n",
        "Since Orbax uses TensorStore as its engine, you can bypass Orbax entirely and read the data directly with TensorStore. This requires you to manually build the `spec` that Orbax would normally create for you.\n",
        "\n",
        "The process has two steps:\n",
        "1.  Treat the checkpoint directory as an `ocdbt` **Key-Value Store** to list all the arrays it contains.\n",
        "2.  Use the name of a specific array to build a full `zarr`-over-`ocdbt` spec to read its data."
      ],
      "metadata": {
        "id": "qBA4U_K1CCco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorstore as ts\n",
        "import os\n",
        "\n",
        "# --- Step 1: List all files and find the unique array names ---\n",
        "\n",
        "# Path to the specific checkpoint step directory created by Orbax\n",
        "ckpt_step_dir = '/tmp/my_orbax_checkpoint/1'\n",
        "\n",
        "print(\"Orbax checkpoint:\")\n",
        "!ls {ckpt_step_dir}\n",
        "\n",
        "# Create a spec for the OCDBT Key-Value Store\n",
        "kvstore_spec = {\n",
        "    'driver': 'ocdbt',\n",
        "    'base': { 'driver': 'file', 'path': ckpt_step_dir }\n",
        "}\n",
        "\n",
        "# Open it as a KvStore and list all file keys\n",
        "kvstore = ts.KvStore.open(kvstore_spec).result()\n",
        "all_file_keys = [key.decode() for key in kvstore.list().result()]\n",
        "\n",
        "print(\"\\nAll Keys found inside the Orbax checkpoint:\")\n",
        "print(all_file_keys)\n",
        "\n",
        "# We need to find the root of each Zarr array by looking\n",
        "# for the '.zarray' metadata files and stripping that suffix.\n",
        "# Orbax also replaces '/' in PyTree names with '.'\n",
        "array_names = sorted(list(set([\n",
        "    key.removesuffix('/.zarray') for key in all_file_keys if key.endswith('.zarray')\n",
        "])))\n",
        "\n",
        "print(\"\\nClean array names found inside the Orbax checkpoint:\")\n",
        "print(array_names)\n",
        "\n",
        "\n",
        "# --- Step 2: Read one specific array using its correct name ---\n",
        "\n",
        "# Let's target the weights of the first layer using its corrected name\n",
        "target_array_name = 'layer1.weights' # CORRECTED: Use '.' instead of '/'\n",
        "\n",
        "# Construct the full spec to read the Zarr array from within the OCDBT store\n",
        "array_spec = {\n",
        "    'driver': 'zarr',\n",
        "    'kvstore': {\n",
        "        'driver': 'ocdbt',\n",
        "        'base': { 'driver': 'file', 'path': ckpt_step_dir },\n",
        "    },\n",
        "    'path': target_array_name,  # Specifies which array to open\n",
        "    'open': True\n",
        "}\n",
        "\n",
        "# Open the specific array using the spec\n",
        "single_array_store = ts.open(array_spec).result()\n",
        "\n",
        "# Read the data into a NumPy array\n",
        "data = single_array_store.read().result()\n",
        "\n",
        "print(f\"\\n--- Reading '{target_array_name}' directly with TensorStore ---\")\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"Data type:\", data.dtype)\n",
        "print(\"A small slice of the data:\\n\", data[0:3, 0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXmBQGMMCDUA",
        "outputId": "f634788b-1f12-43bb-fa25-b7282320f216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orbax checkpoint:\n",
            "array_metadatas       d\t\t      _METADATA        _sharding\n",
            "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n",
            "\n",
            "All Keys found inside the Orbax checkpoint:\n",
            "['layer1.bias/.zarray', 'layer1.bias/0', 'layer1.weights/.zarray', 'layer1.weights/0.0', 'layer2.bias/.zarray', 'layer2.bias/0', 'layer2.weights/.zarray', 'layer2.weights/0.0']\n",
            "\n",
            "Clean array names found inside the Orbax checkpoint:\n",
            "['layer1.bias', 'layer1.weights', 'layer2.bias', 'layer2.weights']\n",
            "\n",
            "--- Reading 'layer1.weights' directly with TensorStore ---\n",
            "Shape: (128, 64)\n",
            "Data type: float32\n",
            "A small slice of the data:\n",
            " [[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bonus: Converting from OCDBT to a Standard Format**\n",
        "\n",
        "You've seen that Orbax uses the highly-optimized `ocdbt` driver for writing checkpoints. While this is fast, `ocdbt` is a specialized format. What if you want to share your checkpoint with a colleague who uses a different tool that only understands standard Zarr, or you want to browse the files manually?\n",
        "\n",
        "You can easily convert the checkpoint to a standard, non-ocdbt format using TensorStore itself! The process involves reading from the `ocdbt` source and writing to a new destination with a simple `file` driver for each array."
      ],
      "metadata": {
        "id": "2wtcExIGGjSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorstore as ts\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# --- Define Source and Destination ---\n",
        "source_ckpt_dir = '/tmp/my_orbax_checkpoint/1'\n",
        "dest_ckpt_dir = '/tmp/my_converted_zarr_checkpoint'\n",
        "\n",
        "# Clean up previous runs\n",
        "if os.path.exists(dest_ckpt_dir):\n",
        "    shutil.rmtree(dest_ckpt_dir)\n",
        "os.makedirs(dest_ckpt_dir)\n",
        "\n",
        "\n",
        "# --- Get the list of array names ---\n",
        "kvstore_spec = {\n",
        "    'driver': 'ocdbt',\n",
        "    'base': { 'driver': 'file', 'path': source_ckpt_dir }\n",
        "}\n",
        "kvstore = ts.KvStore.open(kvstore_spec).result()\n",
        "all_file_keys = [key.decode() for key in kvstore.list().result()]\n",
        "array_names = sorted(list(set([\n",
        "    key.removesuffix('/.zarray') for key in all_file_keys if key.endswith('.zarray')\n",
        "])))\n",
        "\n",
        "print(f\"Found {len(array_names)} arrays to convert.\")\n",
        "\n",
        "\n",
        "# --- Loop through each array and convert it ---\n",
        "for array_name in array_names:\n",
        "    print(f\"Converting '{array_name}'...\")\n",
        "\n",
        "    # 1. Define and open the SOURCE store\n",
        "    source_spec = {\n",
        "        'driver': 'zarr',\n",
        "        'kvstore': {\n",
        "            'driver': 'ocdbt',\n",
        "            'base': { 'driver': 'file', 'path': source_ckpt_dir },\n",
        "        },\n",
        "        'path': array_name\n",
        "    }\n",
        "    source_store = ts.open(source_spec).result()\n",
        "\n",
        "    # 2. Define the DESTINATION spec\n",
        "    dest_spec = {\n",
        "        'driver': 'zarr',\n",
        "        'kvstore': {\n",
        "            'driver': 'file',\n",
        "            'path': os.path.join(dest_ckpt_dir, array_name)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 3. Open the destination store, passing the entire schema from the source.\n",
        "    #    THE FIX: This preserves chunk size, compression, dtype, domain, etc.\n",
        "    dest_store = ts.open(\n",
        "        dest_spec,\n",
        "        create=True,\n",
        "        delete_existing=True,\n",
        "        schema=source_store.schema  # <-- Copies all metadata\n",
        "    ).result()\n",
        "\n",
        "    # 4. Perform the copy\n",
        "    dest_store.write(source_store).result()\n",
        "\n",
        "print(\"\\n‚úÖ Conversion complete with all metadata preserved!\")\n",
        "print(f\"Standard Zarr checkpoint saved at: {dest_ckpt_dir}\")\n",
        "\n",
        "# Let's inspect the new directory structure.\n",
        "!ls -R {dest_ckpt_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFueABORGnZl",
        "outputId": "85bded8a-8f02-445d-c2fe-55ab703ecd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 arrays to convert.\n",
            "Converting 'layer1.bias'...\n",
            "Converting 'layer1.weights'...\n",
            "Converting 'layer2.bias'...\n",
            "Converting 'layer2.weights'...\n",
            "\n",
            "‚úÖ Conversion complete with all metadata preserved!\n",
            "Standard Zarr checkpoint saved at: /tmp/my_converted_zarr_checkpoint\n",
            "/tmp/my_converted_zarr_checkpoint:\n",
            "layer1.bias  layer1.weights  layer2.bias  layer2.weights\n",
            "\n",
            "/tmp/my_converted_zarr_checkpoint/layer1.bias:\n",
            "0\n",
            "\n",
            "/tmp/my_converted_zarr_checkpoint/layer1.weights:\n",
            "0.0\n",
            "\n",
            "/tmp/my_converted_zarr_checkpoint/layer2.bias:\n",
            "0\n",
            "\n",
            "/tmp/my_converted_zarr_checkpoint/layer2.weights:\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if You Want to Customize?\n",
        "\n",
        "While copying the whole schema is great for an exact replica, you can also use this step to change parameters. For example, if you wanted to convert the data and simultaneously change its chunking, you could override just that part of the schema:\n",
        "\n",
        "```python\n",
        "# Example of overriding the chunk layout during conversion\n",
        "# new_chunk_layout = ts.ChunkLayout(write_chunk_shape=[32, 32])\n",
        "# dest_store = ts.open(\n",
        "#     dest_spec,\n",
        "#     create=True,\n",
        "#     schema=source_store.schema,\n",
        "#     chunk_layout=new_chunk_layout  # <-- Override just the chunking\n",
        "# ).result()\n",
        "```"
      ],
      "metadata": {
        "id": "CKpsqJDWIo7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion: The Journey** ‚úÖ\n",
        "\n",
        "Congratulations! You've completed the journey from zero to TensorStore hero.\n",
        "\n",
        "You started by understanding a fundamental problem‚Äîdata being too large for memory. You then learned how TensorStore solves this with its core concepts:\n",
        "\n",
        "* The declarative **`spec`** to define data.\n",
        "* **Chunking** and **asynchronous I/O** for performance.\n",
        "* The **`kvstore`** abstraction for cloud portability.\n",
        "* Powerful **virtual views** and **transactions** for advanced workflows.\n",
        "\n",
        "Finally, you saw how all these concepts come together to power a critical library in the modern ML ecosystem, **Orbax**, turning a complex checkpointing problem into a simple and efficient operation. You now have the foundation to use TensorStore for your own large-scale data challenges."
      ],
      "metadata": {
        "id": "MlYrgSZPs_yu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlXBPXoSIu19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}